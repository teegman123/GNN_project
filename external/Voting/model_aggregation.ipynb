{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ab2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\dev\\school\\CSC591 - MLG\\proj\\cns\\CorrectAndSmooth\\venv\\Lib\\site-packages\\outdated\\__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Fix for PyTorch 2.6+ compatibility with OGB dataset loading\n",
    "# Patch torch.load to use weights_only=False for compatibility with torch_geometric\n",
    "_original_torch_load = torch.load\n",
    "def _patched_torch_load(*args, **kwargs):\n",
    "    # Set weights_only=False if not explicitly provided (for PyTorch 2.6+)\n",
    "    if 'weights_only' not in kwargs:\n",
    "        kwargs['weights_only'] = False\n",
    "    return _original_torch_load(*args, **kwargs)\n",
    "torch.load = _patched_torch_load\n",
    "\n",
    "# Now import OGB after patching torch.load\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "# Configuration\n",
    "dataset_name = 'products'\n",
    "predictions_dir = 'predictions'\n",
    "models_dir = 'models'\n",
    "labels_csv = 'gcn_predictions_2.csv'\n",
    "gcn_pt_file = 'gcn_outputs_2.pt'  # Fallback GCN predictions as .pt tensor file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0caae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and split indices...\n",
      "Test set size: 2213091 nodes\n",
      "\n",
      "Loading true labels...\n",
      "Loaded 2449029 total nodes\n",
      "True labels shape: (2449029,)\n",
      "Test indices range: 235938 to 2449028\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and get split indices (like run_experiments.py)\n",
    "print(\"Loading dataset and split indices...\")\n",
    "dataset = PygNodePropPredDataset(name=f'ogbn-{dataset_name}')\n",
    "data = dataset[0]\n",
    "split_idx = dataset.get_idx_split()\n",
    "test_idx = split_idx['test'].numpy()\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n",
    "\n",
    "# Load true labels\n",
    "print(\"\\nLoading true labels...\")\n",
    "labels_df = pd.read_csv(labels_csv)\n",
    "labels_df = labels_df.sort_values('node_id').reset_index(drop=True)\n",
    "true_labels = labels_df['true_label'].values\n",
    "num_nodes = len(true_labels)\n",
    "print(f\"Loaded {num_nodes} total nodes\")\n",
    "print(f\"True labels shape: {true_labels.shape}\")\n",
    "print(f\"Test indices range: {test_idx.min()} to {test_idx.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d847752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plain_run0: 2449029 predictions\n",
      "Loaded plain_run1: 2449029 predictions\n",
      "Loaded plain_run2: 2449029 predictions\n",
      "Loaded plain_run3: 2449029 predictions\n",
      "Loaded plain_run4: 2449029 predictions\n",
      "Loaded linear_run0: 2449029 predictions\n",
      "Loaded linear_run1: 2449029 predictions\n",
      "Loaded linear_run2: 2449029 predictions\n",
      "Loaded linear_run3: 2449029 predictions\n",
      "Loaded linear_run4: 2449029 predictions\n",
      "Loaded mlp_run0: 2449029 predictions\n",
      "Loaded mlp_run1: 2449029 predictions\n",
      "Loaded mlp_run2: 2449029 predictions\n",
      "Loaded mlp_run3: 2449029 predictions\n",
      "Loaded mlp_run4: 2449029 predictions\n",
      "\n",
      "Loading GCN predictions from .pt files...\n",
      "Models directory models/products_gcn not found, checking for fallback file...\n",
      "Using fallback file: gcn_outputs_2.pt\n",
      "Loaded gcn_run0: 2449029 predictions (with top 3 predictions)\n",
      "\n",
      "Loading C&S predictions from ogbn-products_final/...\n",
      "Found 5 C&S .pt files\n",
      "Loaded gamlp_cs_0919abde: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_cs_09883da0: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_cs_9cd3b467: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_cs_c24dfceb: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_cs_eddd8eb7: 2449029 predictions (with top 3 predictions)\n",
      "\n",
      "Loading GAMLP (non-C&S) predictions from ogbn-products_final/...\n",
      "Found 5 GAMLP (non-C&S) .pt files\n",
      "Loaded gamlp_0919abde: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_09883da0: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_9cd3b467: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_c24dfceb: 2449029 predictions (with top 3 predictions)\n",
      "Loaded gamlp_eddd8eb7: 2449029 predictions (with top 3 predictions)\n",
      "\n",
      "Total models loaded: 26\n"
     ]
    }
   ],
   "source": [
    "# Load all prediction CSV files\n",
    "methods = ['plain', 'linear', 'mlp']\n",
    "all_predictions = {}\n",
    "\n",
    "for method in methods:\n",
    "    method_dir = f'{predictions_dir}/{dataset_name}_{method}'\n",
    "    pred_files = glob.glob(f'{method_dir}/run*_predictions.csv')\n",
    "    pred_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].replace('run', '').replace('_predictions', '')))\n",
    "    \n",
    "    for pred_file in pred_files:\n",
    "        run_num = int(os.path.splitext(os.path.basename(pred_file))[0].replace('run', '').replace('_predictions', ''))\n",
    "        key = f'{method}_run{run_num}'\n",
    "        df = pd.read_csv(pred_file)\n",
    "        df = df.sort_values('node_id').reset_index(drop=True)\n",
    "        all_predictions[key] = df\n",
    "        print(f\"Loaded {key}: {len(df)} predictions\")\n",
    "\n",
    "# Load GCN predictions from .pt files (similar to other models)\n",
    "print(f\"\\nLoading GCN predictions from .pt files...\")\n",
    "gcn_model_dir = f'{models_dir}/{dataset_name}_gcn'\n",
    "gcn_pt_files = []\n",
    "\n",
    "# Try to load from models directory first\n",
    "if os.path.exists(gcn_model_dir):\n",
    "    gcn_pt_files = glob.glob(f'{gcn_model_dir}/*.pt')\n",
    "    gcn_pt_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]) if os.path.splitext(os.path.basename(x))[0].isdigit() else 0)\n",
    "    print(f\"Found {len(gcn_pt_files)} GCN .pt files in {gcn_model_dir}\")\n",
    "else:\n",
    "    print(f\"Models directory {gcn_model_dir} not found, checking for fallback file...\")\n",
    "\n",
    "# If no files in models directory, try fallback file\n",
    "if len(gcn_pt_files) == 0 and os.path.exists(gcn_pt_file):\n",
    "    print(f\"Using fallback file: {gcn_pt_file}\")\n",
    "    gcn_pt_files = [gcn_pt_file]\n",
    "\n",
    "# Load top 3 GCN runs (or all available if less than 3)\n",
    "num_gcn_runs = min(3, len(gcn_pt_files))\n",
    "gcn_pt_files = gcn_pt_files[:num_gcn_runs]\n",
    "\n",
    "for i, gcn_pt_file in enumerate(gcn_pt_files):\n",
    "    # Extract run number from filename, or use index\n",
    "    filename_base = os.path.splitext(os.path.basename(gcn_pt_file))[0]\n",
    "    try:\n",
    "        run_num = int(filename_base)\n",
    "    except ValueError:\n",
    "        # If filename is not a number (e.g., \"gcn_outputs_2\"), use index\n",
    "        run_num = i\n",
    "    key = f'gcn_run{run_num}'\n",
    "    \n",
    "    # Load predictions from .pt file\n",
    "    gcn_predictions = torch.load(gcn_pt_file, map_location='cpu')  # Shape: [num_nodes, num_classes]\n",
    "    \n",
    "    # Get top 3 predictions for each node\n",
    "    top3_values, top3_indices = torch.topk(gcn_predictions, k=3, dim=1)\n",
    "    top3_indices_np = top3_indices.numpy()  # Shape: [num_nodes, 3]\n",
    "    \n",
    "    # Create DataFrame in the same format as other models\n",
    "    gcn_pred_df = pd.DataFrame({\n",
    "        'node_id': np.arange(len(gcn_predictions)),\n",
    "        'prediction_1': top3_indices_np[:, 0],\n",
    "        'prediction_2': top3_indices_np[:, 1],\n",
    "        'prediction_3': top3_indices_np[:, 2],\n",
    "        'truth': true_labels\n",
    "    })\n",
    "    \n",
    "    all_predictions[key] = gcn_pred_df\n",
    "    print(f\"Loaded {key}: {len(gcn_pred_df)} predictions (with top 3 predictions)\")\n",
    "\n",
    "# Load C&S (Correct and Smooth) predictions from ogbn-products_final directory\n",
    "print(f\"\\nLoading C&S predictions from ogbn-products_final/...\")\n",
    "cs_dir = 'ogbn-products_final'\n",
    "cs_pt_files = glob.glob(f'{cs_dir}/*_cs.pt')\n",
    "cs_pt_files.sort()  # Sort alphabetically for consistent ordering\n",
    "\n",
    "print(f\"Found {len(cs_pt_files)} C&S .pt files\")\n",
    "\n",
    "for i, cs_pt_file in enumerate(cs_pt_files):\n",
    "    # Extract a short identifier from the filename (first part of hash)\n",
    "    filename = os.path.basename(cs_pt_file)\n",
    "    # Use first 8 chars of hash as identifier, or just use index\n",
    "    hash_part = filename.split('_')[0]\n",
    "    key = f'gamlp_cs_{hash_part[:8]}'\n",
    "    \n",
    "    # Load predictions from .pt file\n",
    "    cs_predictions = torch.load(cs_pt_file, map_location='cpu')  # Shape: [num_nodes, num_classes]\n",
    "    \n",
    "    # Get top 3 predictions for each node\n",
    "    top3_values, top3_indices = torch.topk(cs_predictions, k=3, dim=1)\n",
    "    top3_indices_np = top3_indices.numpy()  # Shape: [num_nodes, 3]\n",
    "    \n",
    "    # Create DataFrame in the same format as other models\n",
    "    cs_pred_df = pd.DataFrame({\n",
    "        'node_id': np.arange(len(cs_predictions)),\n",
    "        'prediction_1': top3_indices_np[:, 0],\n",
    "        'prediction_2': top3_indices_np[:, 1],\n",
    "        'prediction_3': top3_indices_np[:, 2],\n",
    "        'truth': true_labels\n",
    "    })\n",
    "    \n",
    "    all_predictions[key] = cs_pred_df\n",
    "    print(f\"Loaded {key}: {len(cs_pred_df)} predictions (with top 3 predictions)\")\n",
    "\n",
    "# Load non-C&S GAMLP predictions from ogbn-products_final directory\n",
    "print(f\"\\nLoading GAMLP (non-C&S) predictions from ogbn-products_final/...\")\n",
    "gamlp_pt_files = glob.glob(f'{cs_dir}/*.pt')\n",
    "# Filter out _cs files to get only non-C&S files\n",
    "gamlp_pt_files = [f for f in gamlp_pt_files if '_cs.pt' not in f]\n",
    "gamlp_pt_files.sort()  # Sort alphabetically for consistent ordering\n",
    "\n",
    "print(f\"Found {len(gamlp_pt_files)} GAMLP (non-C&S) .pt files\")\n",
    "\n",
    "for i, gamlp_pt_file in enumerate(gamlp_pt_files):\n",
    "    # Extract a short identifier from the filename (first part of hash)\n",
    "    filename = os.path.basename(gamlp_pt_file)\n",
    "    # Use first 8 chars of hash as identifier\n",
    "    hash_part = filename.split('_')[0]\n",
    "    key = f'gamlp_{hash_part[:8]}'\n",
    "    \n",
    "    # Load predictions from .pt file\n",
    "    gamlp_predictions = torch.load(gamlp_pt_file, map_location='cpu')  # Shape: [num_nodes, num_classes]\n",
    "    \n",
    "    # Get top 3 predictions for each node\n",
    "    top3_values, top3_indices = torch.topk(gamlp_predictions, k=3, dim=1)\n",
    "    top3_indices_np = top3_indices.numpy()  # Shape: [num_nodes, 3]\n",
    "    \n",
    "    # Create DataFrame in the same format as other models\n",
    "    gamlp_pred_df = pd.DataFrame({\n",
    "        'node_id': np.arange(len(gamlp_predictions)),\n",
    "        'prediction_1': top3_indices_np[:, 0],\n",
    "        'prediction_2': top3_indices_np[:, 1],\n",
    "        'prediction_3': top3_indices_np[:, 2],\n",
    "        'truth': true_labels\n",
    "    })\n",
    "    \n",
    "    all_predictions[key] = gamlp_pred_df\n",
    "    print(f\"Loaded {key}: {len(gamlp_pred_df)} predictions (with top 3 predictions)\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(all_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e17a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INDIVIDUAL MODEL ACCURACIES (TEST SET ONLY)\n",
      "============================================================\n",
      "plain_run0          : 0.8253 (82.53%)\n",
      "plain_run1          : 0.8254 (82.54%)\n",
      "plain_run2          : 0.8248 (82.48%)\n",
      "plain_run3          : 0.8250 (82.50%)\n",
      "plain_run4          : 0.8249 (82.49%)\n",
      "linear_run0         : 0.8296 (82.96%)\n",
      "linear_run1         : 0.8297 (82.97%)\n",
      "linear_run2         : 0.8294 (82.94%)\n",
      "linear_run3         : 0.8297 (82.97%)\n",
      "linear_run4         : 0.8297 (82.97%)\n",
      "mlp_run0            : 0.8411 (84.11%)\n",
      "mlp_run1            : 0.8421 (84.21%)\n",
      "mlp_run2            : 0.8422 (84.22%)\n",
      "mlp_run3            : 0.8422 (84.22%)\n",
      "mlp_run4            : 0.8411 (84.11%)\n",
      "gcn_run0            : 0.8539 (85.39%)\n",
      "gamlp_cs_0919abde   : 0.8515 (85.15%)\n",
      "gamlp_cs_09883da0   : 0.8519 (85.19%)\n",
      "gamlp_cs_9cd3b467   : 0.8510 (85.10%)\n",
      "gamlp_cs_c24dfceb   : 0.8514 (85.14%)\n",
      "gamlp_cs_eddd8eb7   : 0.8508 (85.08%)\n",
      "gamlp_0919abde      : 0.8505 (85.05%)\n",
      "gamlp_09883da0      : 0.8508 (85.08%)\n",
      "gamlp_9cd3b467      : 0.8497 (84.97%)\n",
      "gamlp_c24dfceb      : 0.8501 (85.01%)\n",
      "gamlp_eddd8eb7      : 0.8493 (84.93%)\n",
      "============================================================\n",
      "Average individual accuracy: 0.8401 (84.01%)\n",
      "Std dev: 0.0108\n",
      "Test set size: 2213091 nodes\n",
      "\n",
      "============================================================\n",
      "SELECTING BEST MODEL FROM EACH CATEGORY\n",
      "============================================================\n",
      "plain          : plain_run1                (accuracy: 0.8254 (82.54%))\n",
      "linear         : linear_run3               (accuracy: 0.8297 (82.97%))\n",
      "mlp            : mlp_run2                  (accuracy: 0.8422 (84.22%))\n",
      "gcn            : gcn_run0                  (accuracy: 0.8539 (85.39%))\n",
      "gamlp_cs       : gamlp_cs_09883da0         (accuracy: 0.8519 (85.19%))\n",
      "gamlp          : gamlp_09883da0            (accuracy: 0.8508 (85.08%))\n",
      "\n",
      "Selected 6 best models: ['plain_run1', 'linear_run3', 'mlp_run2', 'gcn_run0', 'gamlp_cs_09883da0', 'gamlp_09883da0']\n",
      "Created filtered predictions dictionary with 6 models\n"
     ]
    }
   ],
   "source": [
    "# Calculate individual accuracy scores (ONLY ON TEST SET)\n",
    "# This must be done before selecting best models\n",
    "print(\"=\" * 60)\n",
    "print(\"INDIVIDUAL MODEL ACCURACIES (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate if not already calculated\n",
    "if 'individual_accuracies' not in globals():\n",
    "    individual_accuracies = {}\n",
    "\n",
    "    # Get test set labels\n",
    "    test_labels = true_labels[test_idx]\n",
    "\n",
    "    for key, df in all_predictions.items():\n",
    "        # Get first prediction (prediction_1) and compare to truth\n",
    "        predictions = df['prediction_1'].values\n",
    "        # Only evaluate on test nodes\n",
    "        test_predictions = predictions[test_idx]\n",
    "        correct = (test_predictions == test_labels).sum()\n",
    "        accuracy = correct / len(test_idx)\n",
    "        individual_accuracies[key] = accuracy\n",
    "        print(f\"{key:20s}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average individual accuracy: {np.mean(list(individual_accuracies.values())):.4f} ({np.mean(list(individual_accuracies.values()))*100:.2f}%)\")\n",
    "    print(f\"Std dev: {np.std(list(individual_accuracies.values())):.4f}\")\n",
    "    print(f\"Test set size: {len(test_idx)} nodes\")\n",
    "else:\n",
    "    print(\"Individual accuracies already calculated, skipping...\")\n",
    "\n",
    "# Select best model from each category\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SELECTING BEST MODEL FROM EACH CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define categories and their prefixes\n",
    "categories = {\n",
    "    'plain': 'plain_run',\n",
    "    'linear': 'linear_run',\n",
    "    'mlp': 'mlp_run',\n",
    "    'gcn': 'gcn_run',\n",
    "    'gamlp_cs': 'gamlp_cs_',\n",
    "    'gamlp': 'gamlp_'\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "best_model_keys = []\n",
    "\n",
    "for category, prefix in categories.items():\n",
    "    # Find all models in this category\n",
    "    # Special handling: exclude gamlp_cs_ when processing gamlp\n",
    "    if category == 'gamlp':\n",
    "        category_models = {k: v for k, v in individual_accuracies.items() if k.startswith(prefix) and not k.startswith('gamlp_cs_')}\n",
    "    else:\n",
    "        category_models = {k: v for k, v in individual_accuracies.items() if k.startswith(prefix)}\n",
    "    \n",
    "    if len(category_models) > 0:\n",
    "        # Find the best model (highest accuracy)\n",
    "        best_key = max(category_models.items(), key=lambda x: x[1])[0]\n",
    "        best_accuracy = category_models[best_key]\n",
    "        best_models[category] = best_key\n",
    "        best_model_keys.append(best_key)\n",
    "        print(f\"{category:15s}: {best_key:25s} (accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%))\")\n",
    "    else:\n",
    "        print(f\"{category:15s}: No models found\")\n",
    "\n",
    "print(f\"\\nSelected {len(best_model_keys)} best models: {best_model_keys}\")\n",
    "\n",
    "# Create filtered predictions dictionary with only best models\n",
    "best_predictions = {key: all_predictions[key] for key in best_model_keys}\n",
    "print(f\"Created filtered predictions dictionary with {len(best_predictions)} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834a9ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Individual accuracies are now calculated in Cell 3 before best model selection.\n",
      "If you need to recalculate, run Cell 3 again.\n"
     ]
    }
   ],
   "source": [
    "# This cell is now combined with Cell 3 above\n",
    "# Individual accuracies are calculated in Cell 3 before best model selection\n",
    "# This cell can be skipped or used for additional analysis if needed\n",
    "\n",
    "print(\"Note: Individual accuracies are now calculated in Cell 3 before best model selection.\")\n",
    "print(\"If you need to recalculate, run Cell 3 again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0edf36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENSEMBLE ACCURACY (At least one model correct) - TEST SET ONLY\n",
      "============================================================\n",
      "Ensemble accuracy: 0.9167 (91.67%)\n",
      "Number of test nodes where at least one model is correct: 2028837 / 2213091\n",
      "Number of test nodes where ALL models are wrong: 184254 / 2213091\n"
     ]
    }
   ],
   "source": [
    "# Calculate ensemble accuracy (correct if AT LEAST ONE model got it right) - TEST SET ONLY\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENSEMBLE ACCURACY (At least one model correct) - TEST SET ONLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a matrix: [num_test_nodes, num_models] where each entry is True if that model got it right\n",
    "test_labels = true_labels[test_idx]\n",
    "correct_matrix = np.zeros((len(test_idx), len(all_predictions)), dtype=bool)\n",
    "\n",
    "model_keys = sorted(all_predictions.keys())\n",
    "for i, key in enumerate(model_keys):\n",
    "    df = all_predictions[key]\n",
    "    predictions = df['prediction_1'].values\n",
    "    # Only evaluate on test nodes\n",
    "    test_predictions = predictions[test_idx]\n",
    "    correct_matrix[:, i] = (test_predictions == test_labels)\n",
    "\n",
    "# For each test node, check if at least one model got it correct\n",
    "ensemble_correct = correct_matrix.any(axis=1)\n",
    "ensemble_accuracy = ensemble_correct.sum() / len(test_idx)\n",
    "\n",
    "print(f\"Ensemble accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
    "print(f\"Number of test nodes where at least one model is correct: {ensemble_correct.sum()} / {len(test_idx)}\")\n",
    "print(f\"Number of test nodes where ALL models are wrong: {(~ensemble_correct).sum()} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59c568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL AGREEMENT ANALYSIS (TEST SET ONLY)\n",
      "============================================================\n",
      "Test nodes where 0 models correct: 184254\n",
      "Test nodes where 1 model correct: 62714\n",
      "Test nodes where 2-5 models correct: 26715\n",
      "Test nodes where 6-10 models correct: 39304\n",
      "Test nodes where 11-15 models correct: 57258\n",
      "Test nodes where 16+ models correct: 1842846\n",
      "Test nodes where ALL 26 models correct: 1665271\n",
      "\n",
      "Average number of models correct per test node: 21.84\n",
      "Median number of models correct per test node: 26\n",
      "Total test nodes analyzed: 2213091\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis: How many models agree on each test node?\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL AGREEMENT ANALYSIS (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count how many models got each test node correct\n",
    "num_models_correct_per_node = correct_matrix.sum(axis=1)\n",
    "num_models = len(all_predictions)\n",
    "\n",
    "print(f\"Test nodes where 0 models correct: {(num_models_correct_per_node == 0).sum()}\")\n",
    "print(f\"Test nodes where 1 model correct: {(num_models_correct_per_node == 1).sum()}\")\n",
    "print(f\"Test nodes where 2-5 models correct: {((num_models_correct_per_node >= 2) & (num_models_correct_per_node <= 5)).sum()}\")\n",
    "print(f\"Test nodes where 6-10 models correct: {((num_models_correct_per_node >= 6) & (num_models_correct_per_node <= 10)).sum()}\")\n",
    "print(f\"Test nodes where 11-15 models correct: {((num_models_correct_per_node >= 11) & (num_models_correct_per_node <= 15)).sum()}\")\n",
    "if num_models > 15:\n",
    "    print(f\"Test nodes where 16+ models correct: {(num_models_correct_per_node >= 16).sum()}\")\n",
    "print(f\"Test nodes where ALL {num_models} models correct: {(num_models_correct_per_node == num_models).sum()}\")\n",
    "\n",
    "print(f\"\\nAverage number of models correct per test node: {num_models_correct_per_node.mean():.2f}\")\n",
    "print(f\"Median number of models correct per test node: {np.median(num_models_correct_per_node):.0f}\")\n",
    "print(f\"Total test nodes analyzed: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f4bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "ANALYSIS WITH BEST MODELS ONLY (6 MODELS)\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "INDIVIDUAL MODEL ACCURACIES - BEST MODELS (TEST SET ONLY)\n",
      "============================================================\n",
      "plain_run1               : 0.8254 (82.54%)\n",
      "linear_run3              : 0.8297 (82.97%)\n",
      "mlp_run2                 : 0.8422 (84.22%)\n",
      "gcn_run0                 : 0.8539 (85.39%)\n",
      "gamlp_cs_09883da0        : 0.8519 (85.19%)\n",
      "gamlp_09883da0           : 0.8508 (85.08%)\n",
      "============================================================\n",
      "Average individual accuracy: 0.8423 (84.23%)\n",
      "Std dev: 0.0111\n",
      "Test set size: 2213091 nodes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS WITH BEST MODELS ONLY (6 models total)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS WITH BEST MODELS ONLY (6 MODELS)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate individual accuracy scores for best models (ONLY ON TEST SET)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INDIVIDUAL MODEL ACCURACIES - BEST MODELS (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_individual_accuracies = {}\n",
    "\n",
    "# Get test set labels\n",
    "test_labels = true_labels[test_idx]\n",
    "\n",
    "for key, df in best_predictions.items():\n",
    "    # Get first prediction (prediction_1) and compare to truth\n",
    "    predictions = df['prediction_1'].values\n",
    "    # Only evaluate on test nodes\n",
    "    test_predictions = predictions[test_idx]\n",
    "    correct = (test_predictions == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "    best_individual_accuracies[key] = accuracy\n",
    "    print(f\"{key:25s}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average individual accuracy: {np.mean(list(best_individual_accuracies.values())):.4f} ({np.mean(list(best_individual_accuracies.values()))*100:.2f}%)\")\n",
    "print(f\"Std dev: {np.std(list(best_individual_accuracies.values())):.4f}\")\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f04722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENSEMBLE ACCURACY - BEST MODELS (At least one model correct) - TEST SET ONLY\n",
      "============================================================\n",
      "Ensemble accuracy: 0.9120 (91.20%)\n",
      "Number of test nodes where at least one model is correct: 2018351 / 2213091\n",
      "Number of test nodes where ALL models are wrong: 194740 / 2213091\n"
     ]
    }
   ],
   "source": [
    "# Calculate ensemble accuracy for best models (correct if AT LEAST ONE model got it right) - TEST SET ONLY\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENSEMBLE ACCURACY - BEST MODELS (At least one model correct) - TEST SET ONLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a matrix: [num_test_nodes, num_models] where each entry is True if that model got it right\n",
    "test_labels = true_labels[test_idx]\n",
    "best_correct_matrix = np.zeros((len(test_idx), len(best_predictions)), dtype=bool)\n",
    "\n",
    "best_model_keys_sorted = sorted(best_predictions.keys())\n",
    "for i, key in enumerate(best_model_keys_sorted):\n",
    "    df = best_predictions[key]\n",
    "    predictions = df['prediction_1'].values\n",
    "    # Only evaluate on test nodes\n",
    "    test_predictions = predictions[test_idx]\n",
    "    best_correct_matrix[:, i] = (test_predictions == test_labels)\n",
    "\n",
    "# For each test node, check if at least one model got it correct\n",
    "best_ensemble_correct = best_correct_matrix.any(axis=1)\n",
    "best_ensemble_accuracy = best_ensemble_correct.sum() / len(test_idx)\n",
    "\n",
    "print(f\"Ensemble accuracy: {best_ensemble_accuracy:.4f} ({best_ensemble_accuracy*100:.2f}%)\")\n",
    "print(f\"Number of test nodes where at least one model is correct: {best_ensemble_correct.sum()} / {len(test_idx)}\")\n",
    "print(f\"Number of test nodes where ALL models are wrong: {(~best_ensemble_correct).sum()} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d33db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL AGREEMENT ANALYSIS - BEST MODELS (TEST SET ONLY)\n",
      "============================================================\n",
      "Test nodes where 0 models correct: 194740\n",
      "Test nodes where 1 model correct: 77975\n",
      "Test nodes where 2 models correct: 39385\n",
      "Test nodes where 3 models correct: 51817\n",
      "Test nodes where 4 models correct: 62596\n",
      "Test nodes where 5 models correct: 97146\n",
      "Test nodes where ALL 6 models correct: 1689432\n",
      "\n",
      "Average number of models correct per test node: 5.05\n",
      "Median number of models correct per test node: 6\n",
      "Total test nodes analyzed: 2213091\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis: How many models agree on each test node? - BEST MODELS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL AGREEMENT ANALYSIS - BEST MODELS (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count how many models got each test node correct\n",
    "best_num_models_correct_per_node = best_correct_matrix.sum(axis=1)\n",
    "best_num_models = len(best_predictions)\n",
    "\n",
    "print(f\"Test nodes where 0 models correct: {(best_num_models_correct_per_node == 0).sum()}\")\n",
    "print(f\"Test nodes where 1 model correct: {(best_num_models_correct_per_node == 1).sum()}\")\n",
    "print(f\"Test nodes where 2 models correct: {(best_num_models_correct_per_node == 2).sum()}\")\n",
    "print(f\"Test nodes where 3 models correct: {(best_num_models_correct_per_node == 3).sum()}\")\n",
    "print(f\"Test nodes where 4 models correct: {(best_num_models_correct_per_node == 4).sum()}\")\n",
    "print(f\"Test nodes where 5 models correct: {(best_num_models_correct_per_node == 5).sum()}\")\n",
    "print(f\"Test nodes where ALL {best_num_models} models correct: {(best_num_models_correct_per_node == best_num_models).sum()}\")\n",
    "\n",
    "print(f\"\\nAverage number of models correct per test node: {best_num_models_correct_per_node.mean():.2f}\")\n",
    "print(f\"Median number of models correct per test node: {np.median(best_num_models_correct_per_node):.0f}\")\n",
    "print(f\"Total test nodes analyzed: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcc8398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY - BEST MODELS (TEST SET ONLY)\n",
      "============================================================\n",
      "Total number of models: 6\n",
      "Test set size: 2213091 nodes\n",
      "Individual model accuracy range: 0.8254 - 0.8539\n",
      "Average individual accuracy: 0.8423\n",
      "Ensemble accuracy (at least one correct): 0.9120\n",
      "Improvement over average: 0.0697 (8.27%)\n",
      "Improvement over best individual: 0.0581 (6.81%)\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison - BEST MODELS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY - BEST MODELS (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total number of models: {len(best_predictions)}\")\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n",
    "print(f\"Individual model accuracy range: {min(best_individual_accuracies.values()):.4f} - {max(best_individual_accuracies.values()):.4f}\")\n",
    "print(f\"Average individual accuracy: {np.mean(list(best_individual_accuracies.values())):.4f}\")\n",
    "print(f\"Ensemble accuracy (at least one correct): {best_ensemble_accuracy:.4f}\")\n",
    "print(f\"Improvement over average: {best_ensemble_accuracy - np.mean(list(best_individual_accuracies.values())):.4f} ({((best_ensemble_accuracy / np.mean(list(best_individual_accuracies.values())) - 1) * 100):.2f}%)\")\n",
    "print(f\"Improvement over best individual: {best_ensemble_accuracy - max(best_individual_accuracies.values()):.4f} ({((best_ensemble_accuracy / max(best_individual_accuracies.values()) - 1) * 100):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7d5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY (TEST SET ONLY)\n",
      "============================================================\n",
      "Total number of models: 26\n",
      "Test set size: 2213091 nodes\n",
      "Individual model accuracy range: 0.8248 - 0.8539\n",
      "Average individual accuracy: 0.8401\n",
      "Ensemble accuracy (at least one correct): 0.9167\n",
      "Improvement over average: 0.0766 (9.12%)\n",
      "Improvement over best individual: 0.0629 (7.36%)\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison (TEST SET ONLY)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY (TEST SET ONLY)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total number of models: {len(all_predictions)}\")\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n",
    "print(f\"Individual model accuracy range: {min(individual_accuracies.values()):.4f} - {max(individual_accuracies.values()):.4f}\")\n",
    "print(f\"Average individual accuracy: {np.mean(list(individual_accuracies.values())):.4f}\")\n",
    "print(f\"Ensemble accuracy (at least one correct): {ensemble_accuracy:.4f}\")\n",
    "print(f\"Improvement over average: {ensemble_accuracy - np.mean(list(individual_accuracies.values())):.4f} ({((ensemble_accuracy / np.mean(list(individual_accuracies.values())) - 1) * 100):.2f}%)\")\n",
    "print(f\"Improvement over best individual: {ensemble_accuracy - max(individual_accuracies.values()):.4f} ({((ensemble_accuracy / max(individual_accuracies.values()) - 1) * 100):.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

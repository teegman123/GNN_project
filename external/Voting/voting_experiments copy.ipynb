{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bafd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\dev\\school\\CSC591 - MLG\\proj\\cns\\CorrectAndSmooth\\venv\\Lib\\site-packages\\outdated\\__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "# Fix for PyTorch 2.6+ compatibility with OGB dataset loading\n",
    "# Patch torch.load to use weights_only=False for compatibility with torch_geometric\n",
    "_original_torch_load = torch.load\n",
    "def _patched_torch_load(*args, **kwargs):\n",
    "    # Set weights_only=False if not explicitly provided (for PyTorch 2.6+)\n",
    "    if 'weights_only' not in kwargs:\n",
    "        kwargs['weights_only'] = False\n",
    "    return _original_torch_load(*args, **kwargs)\n",
    "torch.load = _patched_torch_load\n",
    "\n",
    "# Configuration\n",
    "dataset_name = 'products'\n",
    "predictions_dir = 'predictions'\n",
    "models_dir = 'models'\n",
    "labels_csv = 'gcn_predictions_2.csv'\n",
    "gcn_pt_file = 'gcn_outputs_2.pt'\n",
    "cs_dir = 'ogbn-products_final'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf89f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and split indices...\n",
      "Test set size: 2213091 nodes\n",
      "\n",
      "Loading true labels...\n",
      "Loaded 2449029 total nodes\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and get split indices\n",
    "print(\"Loading dataset and split indices...\")\n",
    "dataset = PygNodePropPredDataset(name=f'ogbn-{dataset_name}')\n",
    "data = dataset[0]\n",
    "split_idx = dataset.get_idx_split()\n",
    "test_idx = split_idx['test'].numpy()\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n",
    "\n",
    "# Load true labels\n",
    "print(\"\\nLoading true labels...\")\n",
    "labels_df = pd.read_csv(labels_csv)\n",
    "labels_df = labels_df.sort_values('node_id').reset_index(drop=True)\n",
    "true_labels = labels_df['true_label'].values\n",
    "num_nodes = len(true_labels)\n",
    "print(f\"Loaded {num_nodes} total nodes\")\n",
    "\n",
    "# Get test set labels\n",
    "test_labels = true_labels[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cc6e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions for the 5 best models...\n",
      "Best models: ['plain_run1', 'linear_run3', 'mlp_run2', 'gamlp_cs_09883da0', 'gamlp_09883da0']\n",
      "\n",
      "Loading plain_run1...\n",
      "\n",
      "Loading linear_run3...\n",
      "\n",
      "Loading mlp_run2...\n",
      "\n",
      "Loading gamlp_cs_09883da0...\n",
      "\n",
      "Loading gamlp_09883da0...\n",
      "\n",
      "Loaded 5 models with predictions\n",
      "Loaded 5 models with probabilities\n"
     ]
    }
   ],
   "source": [
    "# Define the 5 best models (from model_aggregation analysis)\n",
    "best_models = {\n",
    "    'plain_run1': 'plain',\n",
    "    'linear_run3': 'linear',\n",
    "    'mlp_run2': 'mlp',\n",
    "    'gamlp_cs_09883da0': 'gamlp_cs',\n",
    "    'gamlp_09883da0': 'gamlp'\n",
    "}\n",
    "\n",
    "print(\"Loading predictions for the 5 best models...\")\n",
    "print(\"Best models:\", list(best_models.keys()))\n",
    "\n",
    "# Dictionary to store predictions (top 3) and probabilities for each model\n",
    "model_predictions = {}  # {model_name: DataFrame with prediction_1, prediction_2, prediction_3}\n",
    "model_probabilities = {}  # {model_name: tensor of shape [num_nodes, num_classes]}\n",
    "\n",
    "# Load predictions for each model\n",
    "for model_name in best_models.keys():\n",
    "    print(f\"\\nLoading {model_name}...\")\n",
    "    \n",
    "    if model_name.startswith('plain_run'):\n",
    "        run_num = int(model_name.replace('plain_run', ''))\n",
    "        pred_file = f'{predictions_dir}/{dataset_name}_plain/run{run_num}_predictions.csv'\n",
    "        df = pd.read_csv(pred_file)\n",
    "        df = df.sort_values('node_id').reset_index(drop=True)\n",
    "        model_predictions[model_name] = df\n",
    "        # Load probabilities from .pt file\n",
    "        pt_file = f'{predictions_dir}/{dataset_name}_plain/run{run_num}.pt'\n",
    "        if os.path.exists(pt_file):\n",
    "            probs = torch.load(pt_file, map_location='cpu')\n",
    "            model_probabilities[model_name] = probs\n",
    "        \n",
    "    elif model_name.startswith('linear_run'):\n",
    "        run_num = int(model_name.replace('linear_run', ''))\n",
    "        pred_file = f'{predictions_dir}/{dataset_name}_linear/run{run_num}_predictions.csv'\n",
    "        df = pd.read_csv(pred_file)\n",
    "        df = df.sort_values('node_id').reset_index(drop=True)\n",
    "        model_predictions[model_name] = df\n",
    "        # Load probabilities from .pt file\n",
    "        pt_file = f'{predictions_dir}/{dataset_name}_linear/run{run_num}.pt'\n",
    "        if os.path.exists(pt_file):\n",
    "            probs = torch.load(pt_file, map_location='cpu')\n",
    "            model_probabilities[model_name] = probs\n",
    "            \n",
    "    elif model_name.startswith('mlp_run'):\n",
    "        run_num = int(model_name.replace('mlp_run', ''))\n",
    "        pred_file = f'{predictions_dir}/{dataset_name}_mlp/run{run_num}_predictions.csv'\n",
    "        df = pd.read_csv(pred_file)\n",
    "        df = df.sort_values('node_id').reset_index(drop=True)\n",
    "        model_predictions[model_name] = df\n",
    "        # Load probabilities from .pt file\n",
    "        pt_file = f'{predictions_dir}/{dataset_name}_mlp/run{run_num}.pt'\n",
    "        if os.path.exists(pt_file):\n",
    "            probs = torch.load(pt_file, map_location='cpu')\n",
    "            model_probabilities[model_name] = probs\n",
    "            \n",
    "    elif model_name.startswith('gcn_run'):\n",
    "        # GCN from fallback file\n",
    "        if os.path.exists(gcn_pt_file):\n",
    "            probs = torch.load(gcn_pt_file, map_location='cpu')\n",
    "            model_probabilities[model_name] = probs\n",
    "            # Create DataFrame from probabilities\n",
    "            top3_values, top3_indices = torch.topk(probs, k=3, dim=1)\n",
    "            df = pd.DataFrame({\n",
    "                'node_id': np.arange(len(probs)),\n",
    "                'prediction_1': top3_indices[:, 0].numpy(),\n",
    "                'prediction_2': top3_indices[:, 1].numpy(),\n",
    "                'prediction_3': top3_indices[:, 2].numpy(),\n",
    "                'truth': true_labels\n",
    "            })\n",
    "            model_predictions[model_name] = df\n",
    "            \n",
    "    elif model_name.startswith('gamlp_cs_'):\n",
    "        # Extract hash from model name\n",
    "        hash_part = model_name.replace('gamlp_cs_', '')\n",
    "        # Find matching file\n",
    "        cs_files = glob.glob(f'{cs_dir}/*_cs.pt')\n",
    "        for cs_file in cs_files:\n",
    "            if hash_part in os.path.basename(cs_file):\n",
    "                probs = torch.load(cs_file, map_location='cpu')\n",
    "                model_probabilities[model_name] = probs\n",
    "                # Create DataFrame from probabilities\n",
    "                top3_values, top3_indices = torch.topk(probs, k=3, dim=1)\n",
    "                df = pd.DataFrame({\n",
    "                    'node_id': np.arange(len(probs)),\n",
    "                    'prediction_1': top3_indices[:, 0].numpy(),\n",
    "                    'prediction_2': top3_indices[:, 1].numpy(),\n",
    "                    'prediction_3': top3_indices[:, 2].numpy(),\n",
    "                    'truth': true_labels\n",
    "                })\n",
    "                model_predictions[model_name] = df\n",
    "                break\n",
    "                \n",
    "    elif model_name.startswith('gamlp_'):\n",
    "        # Extract hash from model name (without cs)\n",
    "        hash_part = model_name.replace('gamlp_', '')\n",
    "        # Find matching file (non-CS)\n",
    "        gamlp_files = glob.glob(f'{cs_dir}/*.pt')\n",
    "        gamlp_files = [f for f in gamlp_files if '_cs.pt' not in f]\n",
    "        for gamlp_file in gamlp_files:\n",
    "            if hash_part in os.path.basename(gamlp_file):\n",
    "                probs = torch.load(gamlp_file, map_location='cpu')\n",
    "                model_probabilities[model_name] = probs\n",
    "                # Create DataFrame from probabilities\n",
    "                top3_values, top3_indices = torch.topk(probs, k=3, dim=1)\n",
    "                df = pd.DataFrame({\n",
    "                    'node_id': np.arange(len(probs)),\n",
    "                    'prediction_1': top3_indices[:, 0].numpy(),\n",
    "                    'prediction_2': top3_indices[:, 1].numpy(),\n",
    "                    'prediction_3': top3_indices[:, 2].numpy(),\n",
    "                    'truth': true_labels\n",
    "                })\n",
    "                model_predictions[model_name] = df\n",
    "                break\n",
    "\n",
    "print(f\"\\nLoaded {len(model_predictions)} models with predictions\")\n",
    "print(f\"Loaded {len(model_probabilities)} models with probabilities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e30627",
   "metadata": {},
   "source": [
    "## Voting Method 1: Basic Voting (1 vote per model for top prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59547fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 1: BASIC VOTING\n",
      "============================================================\n",
      "Each model gets 1 vote for their top prediction\n",
      "\n",
      "Ensemble accuracy (Basic Voting): 0.8457 (84.57%)\n",
      "Correct predictions: 1871600 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 1: BASIC VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Each model gets 1 vote for their top prediction\")\n",
    "print()\n",
    "\n",
    "# Get predictions for test nodes only\n",
    "ensemble_predictions = []\n",
    "\n",
    "for node_idx in test_idx:\n",
    "    # Collect votes from each model\n",
    "    votes = []\n",
    "    for model_name in best_models.keys():\n",
    "        df = model_predictions[model_name]\n",
    "        top_pred = df.loc[node_idx, 'prediction_1']\n",
    "        votes.append(int(top_pred))\n",
    "    \n",
    "    # Count votes and select most common (with tie-breaking)\n",
    "    vote_counts = Counter(votes)\n",
    "    # Get the class with most votes, break ties by selecting the first one\n",
    "    most_common = vote_counts.most_common(1)[0][0]\n",
    "    ensemble_predictions.append(most_common)\n",
    "\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (ensemble_predictions == test_labels).sum()\n",
    "accuracy = correct / len(test_idx)\n",
    "\n",
    "print(f\"Ensemble accuracy (Basic Voting): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091847ea",
   "metadata": {},
   "source": [
    "## Voting Method 2: Ranked Choice Voting (3 points for 1st, 2 for 2nd, 1 for 3rd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e92674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 2: RANKED CHOICE VOTING\n",
      "============================================================\n",
      "Each model gets 3 votes: 3 points for 1st choice, 2 for 2nd, 1 for 3rd\n",
      "\n",
      "Ensemble accuracy (Ranked Choice Voting): 0.8485 (84.85%)\n",
      "Correct predictions: 1877803 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 2: RANKED CHOICE VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Each model gets 3 votes: 3 points for 1st choice, 2 for 2nd, 1 for 3rd\")\n",
    "print()\n",
    "\n",
    "# Get number of classes (from first model's probabilities)\n",
    "num_classes = model_probabilities[list(best_models.keys())[0]].shape[1]\n",
    "\n",
    "ensemble_predictions_ranked = []\n",
    "\n",
    "for node_idx in test_idx:\n",
    "    # Initialize score array for all classes\n",
    "    class_scores = np.zeros(num_classes)\n",
    "    \n",
    "    # Collect votes from each model\n",
    "    for model_name in best_models.keys():\n",
    "        df = model_predictions[model_name]\n",
    "        pred1 = int(df.loc[node_idx, 'prediction_1'])\n",
    "        pred2 = int(df.loc[node_idx, 'prediction_2'])\n",
    "        pred3 = int(df.loc[node_idx, 'prediction_3'])\n",
    "        \n",
    "        # Add points: 3 for 1st, 2 for 2nd, 1 for 3rd\n",
    "        class_scores[pred1] += 3\n",
    "        class_scores[pred2] += 2\n",
    "        class_scores[pred3] += 1\n",
    "    \n",
    "    # Select class with highest score\n",
    "    predicted_class = np.argmax(class_scores)\n",
    "    ensemble_predictions_ranked.append(predicted_class)\n",
    "\n",
    "ensemble_predictions_ranked = np.array(ensemble_predictions_ranked)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (ensemble_predictions_ranked == test_labels).sum()\n",
    "accuracy = correct / len(test_idx)\n",
    "\n",
    "print(f\"Ensemble accuracy (Ranked Choice Voting): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb3dc2",
   "metadata": {},
   "source": [
    "## Voting Method 3: Probability Sum Voting (sum all probabilities, highest wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526977a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 3: PROBABILITY SUM VOTING\n",
      "============================================================\n",
      "Sum all output probabilities across models, highest sum wins\n",
      "\n",
      "Ensemble accuracy (Probability Sum Voting): 0.8518 (85.18%)\n",
      "Correct predictions: 1885028 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 3: PROBABILITY SUM VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Sum all output probabilities across models, highest sum wins\")\n",
    "print()\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_prob = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Sum probabilities across all models\n",
    "        summed_probs = None\n",
    "        \n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            \n",
    "            if summed_probs is None:\n",
    "                summed_probs = probs.clone() if isinstance(probs, torch.Tensor) else probs.copy()\n",
    "            else:\n",
    "                summed_probs += probs\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(summed_probs, torch.Tensor):\n",
    "            summed_probs = summed_probs.numpy()\n",
    "        \n",
    "        # Select class with highest summed probability\n",
    "        predicted_class = np.argmax(summed_probs)\n",
    "        ensemble_predictions_prob.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_prob = np.array(ensemble_predictions_prob)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_prob == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"Ensemble accuracy (Probability Sum Voting): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a811",
   "metadata": {},
   "source": [
    "## Voting Method 4: Proportional Top-5 Voting (5 points split proportionally among top 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8988cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 4: PROPORTIONAL TOP-5 VOTING\n",
      "============================================================\n",
      "Each model gets 5 points, split proportionally among top 5 probabilities\n",
      "\n",
      "Ensemble accuracy (Proportional Top-5 Voting): 0.8524 (85.24%)\n",
      "Correct predictions: 1886520 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 4: PROPORTIONAL TOP-5 VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Each model gets 5 points, split proportionally among top 5 probabilities\")\n",
    "print()\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_prop5 = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Initialize score array for all classes\n",
    "        class_scores = np.zeros(num_classes)\n",
    "        \n",
    "        # Collect votes from each model\n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(probs, torch.Tensor):\n",
    "                probs = probs.numpy()\n",
    "            \n",
    "            # Get top 5 probabilities and their indices\n",
    "            top5_indices = np.argsort(probs)[-5:][::-1]  # Indices of top 5 (descending)\n",
    "            top5_probs = probs[top5_indices]  # Top 5 probabilities\n",
    "            \n",
    "            # Sum of top 5 probabilities\n",
    "            top5_sum = np.sum(top5_probs)\n",
    "            \n",
    "            # Allocate 5 points proportionally\n",
    "            if top5_sum > 0:\n",
    "                # Normalize by sum and multiply by 5\n",
    "                point_allocations = (top5_probs / top5_sum) * 5.0\n",
    "                \n",
    "                # Add points to class scores\n",
    "                for idx, points in zip(top5_indices, point_allocations):\n",
    "                    class_scores[idx] += points\n",
    "        \n",
    "        # Select class with highest score\n",
    "        predicted_class = np.argmax(class_scores)\n",
    "        ensemble_predictions_prop5.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_prop5 = np.array(ensemble_predictions_prop5)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_prop5 == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"Ensemble accuracy (Proportional Top-5 Voting): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ff887",
   "metadata": {},
   "source": [
    "## Voting Method 5: Performance-Weighted Probability Sum (weight by individual model accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c229f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 5: PERFORMANCE-WEIGHTED PROBABILITY SUM\n",
      "============================================================\n",
      "Sum probabilities weighted by each model's individual accuracy\n",
      "\n",
      "Model weights (normalized):\n",
      "  plain_run1               : 0.9826\n",
      "  linear_run3              : 0.9878\n",
      "  mlp_run2                 : 1.0026\n",
      "  gamlp_cs_09883da0        : 1.0142\n",
      "  gamlp_09883da0           : 1.0128\n",
      "\n",
      "Ensemble accuracy (Performance-Weighted): 0.8518 (85.18%)\n",
      "Correct predictions: 1885022 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 5: PERFORMANCE-WEIGHTED PROBABILITY SUM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Sum probabilities weighted by each model's individual accuracy\")\n",
    "print()\n",
    "\n",
    "# Calculate individual model accuracies for weighting\n",
    "model_weights = {}\n",
    "for model_name in best_models.keys():\n",
    "    df = model_predictions[model_name]\n",
    "    test_preds = df.loc[test_idx, 'prediction_1'].values\n",
    "    correct = (test_preds == test_labels).sum()\n",
    "    acc = correct / len(test_idx)\n",
    "    model_weights[model_name] = acc\n",
    "\n",
    "# Normalize weights to sum to number of models (so average weight = 1)\n",
    "total_weight = sum(model_weights.values())\n",
    "num_models = len(model_weights)\n",
    "for model_name in model_weights:\n",
    "    model_weights[model_name] = (model_weights[model_name] / total_weight) * num_models\n",
    "\n",
    "print(\"Model weights (normalized):\")\n",
    "for model_name, weight in model_weights.items():\n",
    "    print(f\"  {model_name:25s}: {weight:.4f}\")\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"\\nWarning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_weighted = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Sum weighted probabilities across all models\n",
    "        summed_probs = None\n",
    "        \n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            weight = model_weights[model_name]\n",
    "            \n",
    "            if summed_probs is None:\n",
    "                summed_probs = (probs * weight).clone() if isinstance(probs, torch.Tensor) else (probs * weight).copy()\n",
    "            else:\n",
    "                summed_probs += probs * weight\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(summed_probs, torch.Tensor):\n",
    "            summed_probs = summed_probs.numpy()\n",
    "        \n",
    "        # Select class with highest summed probability\n",
    "        predicted_class = np.argmax(summed_probs)\n",
    "        ensemble_predictions_weighted.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_weighted = np.array(ensemble_predictions_weighted)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_weighted == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"\\nEnsemble accuracy (Performance-Weighted): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181a689",
   "metadata": {},
   "source": [
    "## Voting Method 6: Confidence-Weighted Voting (weight by model's max probability/confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ddbde36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 6: CONFIDENCE-WEIGHTED VOTING\n",
      "============================================================\n",
      "Weight each model's probabilities by its confidence (max probability) for that node\n",
      "\n",
      "Ensemble accuracy (Confidence-Weighted): 0.8510 (85.10%)\n",
      "Correct predictions: 1883371 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 6: CONFIDENCE-WEIGHTED VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Weight each model's probabilities by its confidence (max probability) for that node\")\n",
    "print()\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_conf = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Sum confidence-weighted probabilities across all models\n",
    "        summed_probs = None\n",
    "        total_confidence = 0.0\n",
    "        \n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(probs, torch.Tensor):\n",
    "                probs_np = probs.numpy()\n",
    "            else:\n",
    "                probs_np = probs\n",
    "            \n",
    "            # Confidence = max probability for this node\n",
    "            confidence = np.max(probs_np)\n",
    "            total_confidence += confidence\n",
    "            \n",
    "            # Weight probabilities by confidence\n",
    "            weighted_probs = probs * confidence\n",
    "            \n",
    "            if summed_probs is None:\n",
    "                summed_probs = weighted_probs.clone() if isinstance(weighted_probs, torch.Tensor) else weighted_probs.copy()\n",
    "            else:\n",
    "                summed_probs += weighted_probs\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(summed_probs, torch.Tensor):\n",
    "            summed_probs = summed_probs.numpy()\n",
    "        \n",
    "        # Select class with highest summed probability\n",
    "        predicted_class = np.argmax(summed_probs)\n",
    "        ensemble_predictions_conf.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_conf = np.array(ensemble_predictions_conf)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_conf == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"Ensemble accuracy (Confidence-Weighted): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc721cf",
   "metadata": {},
   "source": [
    "## Voting Method 7: Geometric Mean Voting (product of probabilities, more robust to outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a3ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 7: GEOMETRIC MEAN VOTING\n",
      "============================================================\n",
      "Use geometric mean (product) of probabilities across models\n",
      "Note: Geometric mean emphasizes agreement between models\n",
      "\n",
      "Ensemble accuracy (Geometric Mean): 0.8518 (85.18%)\n",
      "Correct predictions: 1885081 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 7: GEOMETRIC MEAN VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use geometric mean (product) of probabilities across models\")\n",
    "print(\"Note: Geometric mean emphasizes agreement between models\")\n",
    "print()\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_geom = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Multiply probabilities across all models (geometric mean = product^(1/n))\n",
    "        product_probs = None\n",
    "        \n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(probs, torch.Tensor):\n",
    "                probs = probs.numpy()\n",
    "            \n",
    "            # Add small epsilon to avoid zeros\n",
    "            probs = probs + 1e-10\n",
    "            \n",
    "            if product_probs is None:\n",
    "                product_probs = probs.copy()\n",
    "            else:\n",
    "                product_probs *= probs\n",
    "        \n",
    "        # Take nth root (geometric mean), but argmax is same for product vs geometric mean\n",
    "        # Select class with highest product\n",
    "        predicted_class = np.argmax(product_probs)\n",
    "        ensemble_predictions_geom.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_geom = np.array(ensemble_predictions_geom)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_geom == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"Ensemble accuracy (Geometric Mean): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87186",
   "metadata": {},
   "source": [
    "## Voting Method 8: Exponential Weighted Voting (raise probabilities to power before summing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32bcb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 8: EXPONENTIAL WEIGHTED VOTING\n",
      "============================================================\n",
      "Raise probabilities to a power (e.g., 2) before summing to emphasize high-confidence predictions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_2264\\3230821001.py:34: RuntimeWarning: invalid value encountered in power\n",
      "  exp_probs = np.power(probs, power)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power 1.5: 0.0009 (0.09%)\n",
      "Power 2.0: 0.1078 (10.78%)\n",
      "Power 2.5: 0.0009 (0.09%)\n",
      "Power 3.0: 0.8508 (85.08%)\n",
      "\n",
      "Best power: 3.0\n",
      "Ensemble accuracy (Exponential Weighted, power=3.0): 0.8508 (85.08%)\n",
      "Correct predictions: 1882972 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 8: EXPONENTIAL WEIGHTED VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Raise probabilities to a power (e.g., 2) before summing to emphasize high-confidence predictions\")\n",
    "print()\n",
    "\n",
    "# Try different powers\n",
    "powers_to_try = [1.5, 2.0, 2.5, 3.0]\n",
    "best_power = None\n",
    "best_acc = 0\n",
    "best_predictions = None\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    for power in powers_to_try:\n",
    "        ensemble_predictions_exp = []\n",
    "        \n",
    "        for node_idx in test_idx:\n",
    "            # Sum exponentiated probabilities across all models\n",
    "            summed_probs = None\n",
    "            \n",
    "            for model_name in best_models.keys():\n",
    "                probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "                \n",
    "                # Convert to numpy if needed\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.numpy()\n",
    "                \n",
    "                # Raise to power\n",
    "                exp_probs = np.power(probs, power)\n",
    "                \n",
    "                if summed_probs is None:\n",
    "                    summed_probs = exp_probs.copy()\n",
    "                else:\n",
    "                    summed_probs += exp_probs\n",
    "            \n",
    "            # Select class with highest summed exponentiated probability\n",
    "            predicted_class = np.argmax(summed_probs)\n",
    "            ensemble_predictions_exp.append(predicted_class)\n",
    "\n",
    "        ensemble_predictions_exp = np.array(ensemble_predictions_exp)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (ensemble_predictions_exp == test_labels).sum()\n",
    "        accuracy = correct / len(test_idx)\n",
    "        \n",
    "        print(f\"Power {power:.1f}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_power = power\n",
    "            best_predictions = ensemble_predictions_exp\n",
    "    \n",
    "    if best_predictions is not None:\n",
    "        ensemble_predictions_exp = best_predictions\n",
    "        print(f\"\\nBest power: {best_power:.1f}\")\n",
    "        print(f\"Ensemble accuracy (Exponential Weighted, power={best_power:.1f}): {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "        print(f\"Correct predictions: {(best_predictions == test_labels).sum()} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1e1cf",
   "metadata": {},
   "source": [
    "## Voting Method 9: Hybrid Weighted (combine performance weight + confidence weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953c0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOTING METHOD 9: HYBRID WEIGHTED VOTING\n",
      "============================================================\n",
      "Combine performance-based weights with per-node confidence weights\n",
      "\n",
      "Ensemble accuracy (Hybrid Weighted): 0.8510 (85.10%)\n",
      "Correct predictions: 1883358 / 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VOTING METHOD 9: HYBRID WEIGHTED VOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Combine performance-based weights with per-node confidence weights\")\n",
    "print()\n",
    "\n",
    "# Calculate individual model accuracies for base weighting\n",
    "model_base_weights = {}\n",
    "for model_name in best_models.keys():\n",
    "    df = model_predictions[model_name]\n",
    "    test_preds = df.loc[test_idx, 'prediction_1'].values\n",
    "    correct = (test_preds == test_labels).sum()\n",
    "    acc = correct / len(test_idx)\n",
    "    model_base_weights[model_name] = acc\n",
    "\n",
    "# Normalize base weights\n",
    "total_weight = sum(model_base_weights.values())\n",
    "num_models = len(model_base_weights)\n",
    "for model_name in model_base_weights:\n",
    "    model_base_weights[model_name] = (model_base_weights[model_name] / total_weight) * num_models\n",
    "\n",
    "# Verify all models have probabilities\n",
    "missing_probs = [name for name in best_models.keys() if name not in model_probabilities]\n",
    "if missing_probs:\n",
    "    print(f\"Warning: Missing probabilities for: {missing_probs}\")\n",
    "    print(\"This method requires probability files (.pt) for all models.\")\n",
    "else:\n",
    "    ensemble_predictions_hybrid = []\n",
    "    \n",
    "    for node_idx in test_idx:\n",
    "        # Sum hybrid-weighted probabilities across all models\n",
    "        summed_probs = None\n",
    "        \n",
    "        for model_name in best_models.keys():\n",
    "            probs = model_probabilities[model_name][node_idx, :]  # Shape: [num_classes]\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(probs, torch.Tensor):\n",
    "                probs_np = probs.numpy()\n",
    "            else:\n",
    "                probs_np = probs\n",
    "            \n",
    "            # Base weight from model performance\n",
    "            base_weight = model_base_weights[model_name]\n",
    "            \n",
    "            # Confidence weight from max probability for this node\n",
    "            confidence = np.max(probs_np)\n",
    "            \n",
    "            # Combined weight\n",
    "            combined_weight = base_weight * confidence\n",
    "            \n",
    "            # Weight probabilities\n",
    "            weighted_probs = probs * combined_weight\n",
    "            \n",
    "            if summed_probs is None:\n",
    "                summed_probs = weighted_probs.clone() if isinstance(weighted_probs, torch.Tensor) else weighted_probs.copy()\n",
    "            else:\n",
    "                summed_probs += weighted_probs\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(summed_probs, torch.Tensor):\n",
    "            summed_probs = summed_probs.numpy()\n",
    "        \n",
    "        # Select class with highest summed probability\n",
    "        predicted_class = np.argmax(summed_probs)\n",
    "        ensemble_predictions_hybrid.append(predicted_class)\n",
    "\n",
    "    ensemble_predictions_hybrid = np.array(ensemble_predictions_hybrid)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (ensemble_predictions_hybrid == test_labels).sum()\n",
    "    accuracy = correct / len(test_idx)\n",
    "\n",
    "    print(f\"Ensemble accuracy (Hybrid Weighted): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct} / {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e89e33",
   "metadata": {},
   "source": [
    "## Summary Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db6c31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY: VOTING METHODS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Individual Model Accuracies (for reference):\n",
      "  plain_run1               : 0.8254 (82.54%)\n",
      "  linear_run3              : 0.8297 (82.97%)\n",
      "  mlp_run2                 : 0.8422 (84.22%)\n",
      "  gamlp_cs_09883da0        : 0.8519 (85.19%)\n",
      "  gamlp_09883da0           : 0.8508 (85.08%)\n",
      "\n",
      "  Average individual accuracy: 0.8400 (84.00%)\n",
      "  Best individual accuracy: 0.8519 (85.19%)\n",
      "\n",
      "============================================================\n",
      "Ensemble Results:\n",
      "============================================================\n",
      "1. Basic Voting:                    0.8457 (84.57%)\n",
      "   Improvement over average:       0.0057 (0.68%)\n",
      "   Improvement over best:           -0.0062 (-0.73%)\n",
      "\n",
      "2. Ranked Choice Voting:           0.8485 (84.85%)\n",
      "   Improvement over average:       0.0085 (1.01%)\n",
      "   Improvement over best:           -0.0034 (-0.40%)\n",
      "\n",
      "3. Probability Sum Voting:        0.8518 (85.18%)\n",
      "   Improvement over average:       0.0117 (1.40%)\n",
      "   Improvement over best:           -0.0002 (-0.02%)\n",
      "\n",
      "4. Proportional Top-5 Voting:      0.8524 (85.24%)\n",
      "   Improvement over average:       0.0124 (1.48%)\n",
      "   Improvement over best:           0.0005 (0.06%)\n",
      "\n",
      "5. Performance-Weighted Voting:     0.8518 (85.18%)\n",
      "   Improvement over average:       0.0117 (1.40%)\n",
      "   Improvement over best:           -0.0002 (-0.02%)\n",
      "\n",
      "6. Confidence-Weighted Voting:     0.8510 (85.10%)\n",
      "   Improvement over average:       0.0110 (1.31%)\n",
      "   Improvement over best:           -0.0009 (-0.11%)\n",
      "\n",
      "7. Geometric Mean Voting:          0.8518 (85.18%)\n",
      "   Improvement over average:       0.0118 (1.40%)\n",
      "   Improvement over best:           -0.0002 (-0.02%)\n",
      "\n",
      "8. Exponential Weighted Voting:     0.8508 (85.08%)\n",
      "   Improvement over average:       0.0108 (1.29%)\n",
      "   Improvement over best:           -0.0011 (-0.13%)\n",
      "\n",
      "9. Hybrid Weighted Voting:          0.8510 (85.10%)\n",
      "   Improvement over average:       0.0110 (1.31%)\n",
      "   Improvement over best:           -0.0009 (-0.11%)\n",
      "\n",
      "============================================================\n",
      "BEST METHOD: Proportional Top-5 Voting\n",
      "Accuracy: 0.8524 (85.24%)\n",
      "Improvement over best individual: 0.0005 (0.06%)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Test set size: 2213091 nodes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: VOTING METHODS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Calculate individual model accuracies for reference\n",
    "print(\"Individual Model Accuracies (for reference):\")\n",
    "individual_accs = {}\n",
    "for model_name in best_models.keys():\n",
    "    df = model_predictions[model_name]\n",
    "    test_preds = df.loc[test_idx, 'prediction_1'].values\n",
    "    correct = (test_preds == test_labels).sum()\n",
    "    acc = correct / len(test_idx)\n",
    "    individual_accs[model_name] = acc\n",
    "    print(f\"  {model_name:25s}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "avg_individual = np.mean(list(individual_accs.values()))\n",
    "best_individual = max(individual_accs.values())\n",
    "print(f\"\\n  Average individual accuracy: {avg_individual:.4f} ({avg_individual*100:.2f}%)\")\n",
    "print(f\"  Best individual accuracy: {best_individual:.4f} ({best_individual*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ensemble Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic voting\n",
    "basic_acc = (ensemble_predictions == test_labels).sum() / len(test_idx)\n",
    "print(f\"1. Basic Voting:                    {basic_acc:.4f} ({basic_acc*100:.2f}%)\")\n",
    "print(f\"   Improvement over average:       {basic_acc - avg_individual:.4f} ({((basic_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "print(f\"   Improvement over best:           {basic_acc - best_individual:.4f} ({((basic_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "\n",
    "# Ranked choice voting\n",
    "ranked_acc = (ensemble_predictions_ranked == test_labels).sum() / len(test_idx)\n",
    "print(f\"\\n2. Ranked Choice Voting:           {ranked_acc:.4f} ({ranked_acc*100:.2f}%)\")\n",
    "print(f\"   Improvement over average:       {ranked_acc - avg_individual:.4f} ({((ranked_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "print(f\"   Improvement over best:           {ranked_acc - best_individual:.4f} ({((ranked_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "\n",
    "# Probability sum voting (if available)\n",
    "if 'ensemble_predictions_prob' in locals():\n",
    "    prob_acc = (ensemble_predictions_prob == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n3. Probability Sum Voting:        {prob_acc:.4f} ({prob_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {prob_acc - avg_individual:.4f} ({((prob_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {prob_acc - best_individual:.4f} ({((prob_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n3. Probability Sum Voting:        Not available (missing probability files)\")\n",
    "\n",
    "# Proportional top-5 voting (if available)\n",
    "if 'ensemble_predictions_prop5' in locals():\n",
    "    prop5_acc = (ensemble_predictions_prop5 == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n4. Proportional Top-5 Voting:      {prop5_acc:.4f} ({prop5_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {prop5_acc - avg_individual:.4f} ({((prop5_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {prop5_acc - best_individual:.4f} ({((prop5_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n4. Proportional Top-5 Voting:      Not available (missing probability files)\")\n",
    "\n",
    "# Performance-weighted voting (if available)\n",
    "if 'ensemble_predictions_weighted' in locals():\n",
    "    weighted_acc = (ensemble_predictions_weighted == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n5. Performance-Weighted Voting:     {weighted_acc:.4f} ({weighted_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {weighted_acc - avg_individual:.4f} ({((weighted_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {weighted_acc - best_individual:.4f} ({((weighted_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n5. Performance-Weighted Voting:     Not available (missing probability files)\")\n",
    "\n",
    "# Confidence-weighted voting (if available)\n",
    "if 'ensemble_predictions_conf' in locals():\n",
    "    conf_acc = (ensemble_predictions_conf == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n6. Confidence-Weighted Voting:     {conf_acc:.4f} ({conf_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {conf_acc - avg_individual:.4f} ({((conf_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {conf_acc - best_individual:.4f} ({((conf_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n6. Confidence-Weighted Voting:     Not available (missing probability files)\")\n",
    "\n",
    "# Geometric mean voting (if available)\n",
    "if 'ensemble_predictions_geom' in locals():\n",
    "    geom_acc = (ensemble_predictions_geom == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n7. Geometric Mean Voting:          {geom_acc:.4f} ({geom_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {geom_acc - avg_individual:.4f} ({((geom_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {geom_acc - best_individual:.4f} ({((geom_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n7. Geometric Mean Voting:          Not available (missing probability files)\")\n",
    "\n",
    "# Exponential weighted voting (if available)\n",
    "if 'ensemble_predictions_exp' in locals():\n",
    "    exp_acc = (ensemble_predictions_exp == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n8. Exponential Weighted Voting:     {exp_acc:.4f} ({exp_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {exp_acc - avg_individual:.4f} ({((exp_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {exp_acc - best_individual:.4f} ({((exp_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n8. Exponential Weighted Voting:     Not available (missing probability files)\")\n",
    "\n",
    "# Hybrid weighted voting (if available)\n",
    "if 'ensemble_predictions_hybrid' in locals():\n",
    "    hybrid_acc = (ensemble_predictions_hybrid == test_labels).sum() / len(test_idx)\n",
    "    print(f\"\\n9. Hybrid Weighted Voting:          {hybrid_acc:.4f} ({hybrid_acc*100:.2f}%)\")\n",
    "    print(f\"   Improvement over average:       {hybrid_acc - avg_individual:.4f} ({((hybrid_acc / avg_individual - 1) * 100):.2f}%)\")\n",
    "    print(f\"   Improvement over best:           {hybrid_acc - best_individual:.4f} ({((hybrid_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n9. Hybrid Weighted Voting:          Not available (missing probability files)\")\n",
    "\n",
    "# Find best method\n",
    "all_methods = {\n",
    "    'Basic Voting': 'ensemble_predictions',\n",
    "    'Ranked Choice Voting': 'ensemble_predictions_ranked',\n",
    "    'Probability Sum Voting': 'ensemble_predictions_prob',\n",
    "    'Proportional Top-5 Voting': 'ensemble_predictions_prop5',\n",
    "    'Performance-Weighted Voting': 'ensemble_predictions_weighted',\n",
    "    'Confidence-Weighted Voting': 'ensemble_predictions_conf',\n",
    "    'Geometric Mean Voting': 'ensemble_predictions_geom',\n",
    "    'Exponential Weighted Voting': 'ensemble_predictions_exp',\n",
    "    'Hybrid Weighted Voting': 'ensemble_predictions_hybrid'\n",
    "}\n",
    "\n",
    "best_method_name = None\n",
    "best_method_acc = 0\n",
    "for method_name, var_name in all_methods.items():\n",
    "    if var_name in locals():\n",
    "        acc = (locals()[var_name] == test_labels).sum() / len(test_idx)\n",
    "        if acc > best_method_acc:\n",
    "            best_method_acc = acc\n",
    "            best_method_name = method_name\n",
    "\n",
    "if best_method_name:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"BEST METHOD: {best_method_name}\")\n",
    "    print(f\"Accuracy: {best_method_acc:.4f} ({best_method_acc*100:.2f}%)\")\n",
    "    print(f\"Improvement over best individual: {best_method_acc - best_individual:.4f} ({((best_method_acc / best_individual - 1) * 100):.2f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Test set size: {len(test_idx)} nodes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
